{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed1bc3b7-7876-4e41-997c-a5e9b7939ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import matplotlib\n",
    "\n",
    "df = pd.read_excel('ins.xlsx')\n",
    "df.rename(columns={\"Unnamed: 3\": \"beat_counter\", \"Unnamed: 4\": \"defect_counter\"}, inplace=True)\n",
    "df.beat_counter = df.beat_counter.fillna('1')\n",
    "df.defect_counter = df.defect_counter.fillna('1')\n",
    "\n",
    "df.loc[(df.defect_counter =='?')|(df.defect_counter =='!'), 'defect_counter'] = '1' \n",
    "\n",
    "df.loc[123, 'end'] = 620320\n",
    "df.loc[126, 'end'] = 3590000\n",
    "\n",
    "ind = df.loc[df.file == '00004-2022-08-02-07-43-27_channel_3.bin'].index[-1]\n",
    "df.loc[ind, 'file'] = '00004-2022-08-02-07-43-27_channel_10.bin'\n",
    "\n",
    "df.file = df.file.apply(lambda x: x[6:])\n",
    "df.loc[df.file == '2022-08-02-07-43-27_channel_3.bin', 'file'] = '2022-08-02-07-43-27_channel_03.bin'\n",
    "\n",
    "print(df.defect_counter.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8992d3a1-9426-432f-a4f8-1c7355ca755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_examples(audio_path, win_len=2.56, hop_len=1.0, sr=112000):\n",
    "    \"\"\"\n",
    "    Constructs audio examples of window length and step (hop_len).\n",
    "    \n",
    "    Parameters:\n",
    "        audio_path (str): Path to the audio file.\n",
    "        win_len (float): Window length in seconds.\n",
    "        hop_len (float): Hop length in seconds.\n",
    "        sr (int): Sample rate of the audio.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple: A tuple containing an array of audio examples and corresponding window ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read audio file\n",
    "    audio = np.fromfile(audio_path, dtype='uint8')\n",
    "\n",
    "    # Convert window and hop lengths from seconds to samples\n",
    "    win_len_samples = int(sr * win_len)\n",
    "    hop_len_samples = int(sr * hop_len)\n",
    "\n",
    "    # Calculate the number of hops\n",
    "    no_of_hops = math.ceil((len(audio) - win_len_samples) / hop_len_samples)\n",
    "\n",
    "    # Pad audio to fit exact number of windows\n",
    "    audio_padded = np.pad(audio, (0, win_len_samples + hop_len_samples * no_of_hops - len(audio)), mode='constant')\n",
    "\n",
    "    # Generate examples\n",
    "    a_ex = [audio_padded[i: i + win_len_samples] for i in range(0, len(audio_padded) - win_len_samples + 1, hop_len_samples)]\n",
    "\n",
    "    # Calculate window ranges in seconds\n",
    "    win_ranges = [(i / sr, (i + win_len_samples) / sr) for i in range(0, len(audio_padded) - win_len_samples + 1, hop_len_samples)]\n",
    "\n",
    "    return a_ex, win_ranges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e9e5ff-52e0-422b-bbd5-400a903a1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_labels(annotation_df, win_start, win_end, win_len, sr=112000):\n",
    "    \"\"\"\n",
    "    Calculates all events in the specified window.\n",
    "    \n",
    "    Parameters:\n",
    "        annotation_df (pd.DataFrame): DataFrame containing audio annotations.\n",
    "        win_start (float): Start of the window in seconds.\n",
    "        win_end (float): End of the window in seconds.\n",
    "        win_len (float): Length of the window in seconds.\n",
    "        sr (int): Sample rate of the audio.\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of events, each represented as [start, end, class] in seconds.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert start and end times in the DataFrame from samples to seconds\n",
    "    start_times_sec = annotation_df['start'] / sr\n",
    "    end_times_sec = annotation_df['end'] / sr\n",
    "\n",
    "    # Initialize an empty list for events that occur in the specified window\n",
    "    events_in_window = []\n",
    "\n",
    "    # Iterate over each annotation\n",
    "    for start, end, event_class in zip(start_times_sec, end_times_sec, annotation_df['defect_counter']):\n",
    "        # Check if the event is within the specified window\n",
    "        if start < win_end and end > win_start:\n",
    "            # Adjust the start and end times of the event relative to the window\n",
    "            adjusted_start = max(start, win_start) - win_start\n",
    "            adjusted_end = min(end, win_end) - win_start\n",
    "\n",
    "            # Append the adjusted event to the list\n",
    "            events_in_window.append([adjusted_start, adjusted_end, event_class])\n",
    "\n",
    "    # If no events are found, append a 'silent' event spanning the whole window\n",
    "    if len(events_in_window) == 0:\n",
    "        events_in_window.append([0, win_len, '0'])\n",
    "\n",
    "    return events_in_window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c4fed7-a67b-4b23-b26d-08be1158c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_universal_labels(events, class_dict, ex_length=10.0, no_of_div=32):\n",
    "    \"\"\"\n",
    "    Generates labels for each divided part of the window.\n",
    "    \n",
    "    Parameters:\n",
    "        events (list): List of events in the window.\n",
    "        class_dict (dict): Dictionary of classes.\n",
    "        ex_length (float): Length of the incoming window.\n",
    "        no_of_div (int): Number of divisions in the window.\n",
    "    \n",
    "    Returns:\n",
    "        np.array: Array of labels for each part of the window.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the length of each segment\n",
    "    segment_length = ex_length / no_of_div\n",
    "\n",
    "    # Initialize the label array\n",
    "    labels = np.zeros((no_of_div, len(class_dict) * 3))\n",
    "\n",
    "    # Process each event\n",
    "    for event in events:\n",
    "        event_start, event_end, event_class = event\n",
    "        event_class_idx = class_dict[event_class] * 3\n",
    "\n",
    "        # Determine the start and end segments for the event\n",
    "        start_segment = int(event_start // segment_length)\n",
    "        end_segment = int(event_end // segment_length)\n",
    "\n",
    "        # Label the segments\n",
    "        for seg in range(start_segment, min(end_segment + 1, no_of_div)):\n",
    "            segment_start = seg * segment_length\n",
    "            segment_end = segment_start + segment_length\n",
    "            labels[seg, event_class_idx] = 1  # Mark the presence of the class\n",
    "            labels[seg, event_class_idx + 1] = max(event_start - segment_start, 0) / segment_length  # Start position in segment\n",
    "            labels[seg, event_class_idx + 2] = min(event_end - segment_start, segment_length) / segment_length  # End position in segment\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42aaf5-9bce-4343-939e-ee00c65b0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unique file names from the dataframe\n",
    "all_files = df.file.unique()\n",
    "\n",
    "# Defining the base folder path where the files are located\n",
    "folder_path = 'Marked data/'\n",
    "\n",
    "# A common prefix to be added to each file name\n",
    "prefix = '00004-'\n",
    "\n",
    "# Constructing the full file paths\n",
    "all_files = [folder_path + prefix + file[:-15] + '/' + prefix + file for file in all_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1aad70-5968-487f-a42f-137776828a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_excel('Marked data/part2/ins.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2998dd4-30d3-4a23-8940-9cc04a9d698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = df1.file.unique()\n",
    "print(all_files)\n",
    "folder_path = 'Marked data/part2/'\n",
    "#prefix = '00004-'\n",
    "all_files = [folder_path+file for file in all_files]\n",
    "all_files, len(all_files), len('Marked data/00004-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281248ef-cef8-4071-bdaa-8e293259604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[66, ['start', 'end']] = [503484, 755824]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b5d6b7-062e-458f-82e9-dc0c97b7c592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={\"Сколько импульсов\": \"beat_counter\", \"Сколько дефектов\": \"defect_counter\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae40556-d8db-470e-bbbb-34788ebe7d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.defect_counter = df1.defect_counter.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf02ab-1382-4eec-bbf6-365168253828",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, audio in enumerate(all_files):\n",
    "    annotation_df = df1.loc[df1.file == audio[len(folder_path):]]\n",
    "    \n",
    "    a, win_ranges = construct_examples(audio, win_len=win_len, hop_len=hop_len) # возвращаем массив окон для ффт и отрезки в секундах\n",
    "    a_new_ex_train += a   # добавляем отрезки в массив\n",
    "    \n",
    "    for w in win_ranges:\n",
    "        # для каждого отрезка пишем события, которые попали в отрезок\n",
    "        labels_t = construct_labels(annotation_df, w[0], w[1], win_len=win_len)\n",
    "        ll = get_universal_labels(labels_t, binary_class_dict, ex_length=win_len, no_of_div = 9)\n",
    "        # потом отрезок делим на бины и записываем какие события попали в бины\n",
    "        # ll = to_seg_by_class(labels_t, class_dict)\n",
    "        a_new_labels_train.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47e463-11bc-4fdf-ac12-10cc7a6dced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceea2fe-0d0d-420b-bbf3-44b928d2959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_null_indexes = []\n",
    "for i, label in enumerate(a_new_labels_train):\n",
    "    if np.sum(label[:, 3:]) != 0:\n",
    "        not_null_indexes.append(i)\n",
    "    \n",
    "len(not_null_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5609f220-740e-4313-ab8b-9d75aeef6c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = np.array(a_new_ex_train)[not_null_indexes]\n",
    "train_labels  = np.array(a_new_labels_train)[not_null_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74088e6-ac1c-442e-91f6-a6d7b7fbf536",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "sr = 112000\n",
    "def get_log_melspectrogram(audio, sr = sr, hop_length = 1120, win_length = 4480, n_fft = 4480, n_mels = 70, fmin = 0, fmax = 56000):\n",
    "    \"\"\"Return the log-scaled Mel bands of an audio signal.\"\"\"\n",
    "    audio_2 = librosa.util.normalize(audio)\n",
    "    bands = librosa.feature.melspectrogram(\n",
    "        y=audio_2, sr=sr, hop_length=hop_length, win_length = win_length, n_fft=n_fft, n_mels=n_mels)\n",
    "    return librosa.core.power_to_db(bands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4cb9b-c70d-4000-9924-95d8ca386e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(train_samples):\n",
    "    M = get_log_melspectrogram(a).T\n",
    "    np.save(\"custom_data/train_data/ex-\" + str(i) + \".npy\", M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31103a6a-2489-4eec-b82f-164eac92114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, a in enumerate(train_labels):\n",
    "    np.save(\"custom_data/train_data/label-\" + str(i) + \".npy\", a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1405d9c0-5976-4dff-9ab9-dc5370a7b76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tryint(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError:\n",
    "        return s\n",
    "    \n",
    "def alphanum_key(s):\n",
    "    \"\"\" Turn a string into a list of string and number chunks.\n",
    "        \"z23a\" -> [\"z\", 23, \"a\"]\n",
    "    \"\"\"\n",
    "    return [ tryint(c) for c in re.split('([0-9]+)', s) ]\n",
    "\n",
    "def sort_nicely(l):\n",
    "    \"\"\" Sort the given list in the way that humans expect.\n",
    "    \"\"\"\n",
    "    l.sort(key=alphanum_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f798dca5-c40d-42e4-a2c4-967c11d7bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\"\"\"\n",
    "Load the individual numpy arrays into partition\n",
    "\"\"\"\n",
    "data = glob.glob(\"custom_data/train_data/ex-*.npy\")  \n",
    "sort_nicely(data)\n",
    "\n",
    "labels = glob.glob(\"custom_data/train_data/label-*.npy\") \n",
    "sort_nicely(labels)\n",
    "\n",
    "train_examples = [(data[i], labels[i]) for i in range(len(data))]\n",
    "\n",
    "random.seed(4)\n",
    "random.shuffle(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf53f46-e586-4fc7-b765-be314a478e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SedDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_examples, sr=sr):\n",
    "        self.train_examples = train_examples\n",
    "        self.sr = sr\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X = np.load(self.train_examples[idx][0])\n",
    "        y = np.load(self.train_examples[idx][1])\n",
    "        \n",
    "        return torch.from_numpy(X), torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed11032e-aa51-47c9-bd2c-69d74686a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SedDataset(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21ca617-00df-4929-94c2-12dc419996b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be4b67-dc59-4f9f-9f51-6598ea3aae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (X1, y1) in enumerate(dataloader):\n",
    "    print(X1.shape, y1.shape)\n",
    "    print(torch.sum(y1[:,:,3:]))\n",
    "    if i>6:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232476b8-8083-47cf-afc3-2c2d5b9f478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf92514-5997-4f52-9612-22b691863ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_yoho_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    my custom loss for yoho task\n",
    "    \"\"\"\n",
    "    \n",
    "    squared_difference = torch.square(y_true - y_pred) #[batch, 9, 12]\n",
    "    #print(squared_difference.shape)\n",
    "    ss_True = squared_difference[:, :, 0] * 0 + 1 #[batch, 9] of ones\n",
    "    ss_0 = y_true[:, :, 0] #[batch, 9]\n",
    "    ss_1 = y_true[:, :, 3]\n",
    "    \n",
    "    sss = torch.stack((ss_True, ss_0, ss_0,\n",
    "                  ss_True, ss_1, ss_1), axis = 2)\n",
    "    \n",
    "    squared_difference =  torch.multiply(squared_difference, sss)\n",
    "    return torch.mean(squared_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1793fd2-2a51-4bf4-9597-2b4ed2c04c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv2D(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size=3, stride = 1, padding=0):\n",
    "        super(DepthwiseSeparableConv2D, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, stride=stride,padding=padding)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242cdcb-8b60-470e-84d4-8efb72379a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.my_shape = args\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.my_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7cecd6-73ac-461b-bdce-308712c6ca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 2),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(32, 32, kernel_size=3),\n",
    "#            nn.Conv2d(32, 64, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(32, 64, 3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(64, 64, kernel_size=3, stride = 2),\n",
    "#            nn.Conv2d(64, 128, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(64, 128, 3, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(128, 128, kernel_size=3),\n",
    "#            nn.Conv2d(128, 256, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(128, 256, 3),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(256, 256, kernel_size=3, stride = 2),\n",
    "#            nn.Conv2d(256, 512, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(256, 512, 3, stride=2),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(512, 512, kernel_size=3),\n",
    "#            nn.Conv2d(512, 1024, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(512, 1024, 3),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(1024, 1024, kernel_size=3, padding = 1),\n",
    "#           nn.Conv2d(1024, 512, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(1024, 512, 3, padding = 1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(512, 512, kernel_size=3, padding = 1),\n",
    "#            nn.Conv2d(512, 256, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(512, 256, 3, padding = 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "#            nn.Conv2d(256, 256, kernel_size=3, padding = 1),\n",
    "#            nn.Conv2d(256, 128, kernel_size=1),\n",
    "            DepthwiseSeparableConv2D(256, 128, 3, padding = 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            Reshape((-1, 512, 27)), \n",
    "            nn.Conv1d(512, 128, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(128, 64, kernel_size=7),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 32, kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 16, kernel_size=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(16, 9, kernel_size=4)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc6c8aa-db66-4eca-ade3-bb0f54471f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in dataloader:\n",
    "    print(x.shape)\n",
    "    i = x.reshape([-1, 1, 257, 70]).float()\n",
    "    i = model(i)\n",
    "    print(i.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c6dc4b-32f5-4bdc-a6e2-d901eac24f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_files, val_files, model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_files, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_files, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    model.to(device)\n",
    "    training_loss = []\n",
    "    val_loss = [] \n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (X, y) in enumerate(train_loader, 0):\n",
    "            model.train()\n",
    "\n",
    "            # get the inputs\n",
    "            images = X.to(device).float()\n",
    "            labels = y.to(device).float()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # predict classes using images from the training set\n",
    "            outputs = model(images.reshape([-1, 1, 257, 70]))\n",
    "            # compute the loss based on model output and real labels\n",
    "            loss = binary_yoho_loss(labels, outputs)\n",
    "            # backpropagate the loss\n",
    "            loss.backward()\n",
    "            # adjust parameters based on the calculated gradients\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.cpu().detach().numpy()/batch_size\n",
    "            \n",
    "        training_loss.append(running_loss)    # extract the loss value\n",
    "\n",
    " \n",
    "        val_running = 0.0\n",
    "        for i, (X, y) in enumerate(val_loader, 0):\n",
    "            model.eval()\n",
    "\n",
    "            val_images = X.to(device).float()\n",
    "            val_labels = y.to(device).float()\n",
    "\n",
    "            out = model(val_images.reshape([-1, 1, 257, 70]))\n",
    "            \n",
    "            val_running += binary_yoho_loss(val_labels, out).cpu().detach().numpy()/batch_size\n",
    "            \n",
    "        val_loss.append(val_running)\n",
    "        \n",
    "       # clear_output(wait=True)\n",
    "       # plt.plot(running_loss, label='train_loss')\n",
    "       # plt.plot(val_loss,label='val_loss')\n",
    "       # plt.legend()\n",
    "       # plt.show\n",
    "    \n",
    "    return training_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd17af5c-ec3a-426d-8549-d9c41a282e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09f7d6-35ce-4c2a-bbef-e1761ae0fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files, val_files = train_test_split(dataset, test_size=0.15)\n",
    "train_loss, val_loss = train(train_files, val_files, model, 60, 32)\n",
    "plt.plot(train_loss[5:], label='train_loss')\n",
    "plt.plot(val_loss[5:],label='val_loss')\n",
    "plt.legend()\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
